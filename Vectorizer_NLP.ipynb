{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "gutenberg_texts = nltk.corpus.gutenberg.fileids()\n",
        "print(gutenberg_texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCIK1owcjsOQ",
        "outputId": "cbcf4457-d7cb-4879-a281-7f7f85a30b66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize, pos_tag\n",
        "sentence = \"Mary is driving a big car\"\n",
        "tokens = word_tokenize(sentence)\n",
        "pos_tags = pos_tag(tokens)\n",
        "for word, tag in pos_tags:\n",
        "  print(f\"{word}: {tag}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZ23Isg8jsXA",
        "outputId": "3650659c-d06e-44a5-d4e9-41d98b7039b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mary: NNP\n",
            "is: VBZ\n",
            "driving: VBG\n",
            "a: DT\n",
            "big: JJ\n",
            "car: NN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#sent = \"Mary is driving a big car.\"\n",
        "sent = \"Apple is a fruit and Apple is a Company's name\"\n",
        "import nltk\n",
        "import nltk.corpus\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5pXAMMbjsey",
        "outputId": "a679e099-5515-432b-bfd8-e9a8869d632d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0GNy3qNjsiJ",
        "outputId": "4b3e0bfa-d5f0-45fd-84ea-75457ed21446"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent_tokens = word_tokenize(sent)"
      ],
      "metadata": {
        "id": "0Syq66zPjslu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sent_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mydEYkfjspQ",
        "outputId": "67efc2f6-fe58-4a9e-cf9a-f1b2a8f5d401"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Apple', 'is', 'a', 'fruit', 'and', 'Apple', 'is', 'a', 'Company', \"'s\", 'name']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for token in sent_tokens:\n",
        "  print(nltk.pos_tag([token]))\n",
        "\n",
        "NE_tags = nltk.pos_tag([token])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8QBtOSdjstF",
        "outputId": "f34a390e-1b7f-4560-a205-1662a2acdce9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Apple', 'NNP')]\n",
            "[('is', 'VBZ')]\n",
            "[('a', 'DT')]\n",
            "[('fruit', 'NN')]\n",
            "[('and', 'CC')]\n",
            "[('Apple', 'NNP')]\n",
            "[('is', 'VBZ')]\n",
            "[('a', 'DT')]\n",
            "[('Company', 'NN')]\n",
            "[(\"'s\", 'POS')]\n",
            "[('name', 'NN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import ne_chunk\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "NE_NER = ne_chunk(NE_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-tI4FTtjsxT",
        "outputId": "9d0eb7c0-d5bf-4338-c8ce-3d41014d2345"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(NE_NER)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KqIU4mpLjs6x",
        "outputId": "7e4c236b-8cfc-4e31-e482-e77895c42e32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S name/NN)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: write python word to remove stop words in nltk\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "sentence = \"Mary is driving a big car.\"\n",
        "words = word_tokenize(sentence)\n",
        "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "print(filtered_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCaolYrjzoKM",
        "outputId": "5b3611a3-006b-4e74-e732-bea7c98bf8be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Mary', 'driving', 'big', 'car', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: remove the stop words\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "sentence = \"Mary is driving a big car.\"\n",
        "words = word_tokenize(sentence)\n",
        "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "print(filtered_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arv32Kvr1AUU",
        "outputId": "fa8067ca-aaf4-4291-94b7-ff62364bc7cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Mary', 'driving', 'big', 'car', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [\n",
        "    \"This is the first document.\",\n",
        "    \"This document is the second document.\",\n",
        "    \"And this is the third one.\",\n",
        "    \"Is this the first document?\",\n",
        "]"
      ],
      "metadata": {
        "id": "H2E63A362tPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download stopwords and punkt tokenizer if not already downloaded\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Sample documents\n",
        "documents = [\n",
        "    \"This is the first document.\",\n",
        "    \"This document is the second document.\",\n",
        "    \"And this is the third one.\",\n",
        "    \"Is this the first document?\",\n",
        "]\n",
        "\n",
        "# Tokenized documents list\n",
        "tokenized_documents = []\n",
        "\n",
        "# Remove stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Iterate through documents\n",
        "for doc in documents:\n",
        "    # Tokenize document\n",
        "    words = word_tokenize(doc)\n",
        "\n",
        "    # Convert tokens to lowercase and remove stopwords using for loop\n",
        "    filtered_words = []\n",
        "    for word in words:\n",
        "        if word.lower() not in stop_words:\n",
        "            filtered_words.append(word.lower())\n",
        "\n",
        "    # Add filtered tokens to tokenized documents list\n",
        "    tokenized_documents.append(filtered_words)\n",
        "\n",
        "# Print tokenized documents\n",
        "for i, doc in enumerate(tokenized_documents):\n",
        "    print(f\"Document {i + 1}: {doc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XIEYwpM6CDN",
        "outputId": "a083fb8a-a8a9-4b2d-f79a-81c4cb1eff6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document 1: ['first', 'document', '.']\n",
            "Document 2: ['document', 'second', 'document', '.']\n",
            "Document 3: ['third', 'one', '.']\n",
            "Document 4: ['first', 'document', '?']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a list of the unique words in the documents\n",
        "# Create a vocabulary (list of unique words)\n",
        "filtered_documents = [[word for word in doc if word not in stop_words] for doc in tokenized_documents]\n",
        "vocabulary = list(set(word for doc in filtered_documents for word in doc))\n",
        "print(vocabulary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRhaNO8k6CGW",
        "outputId": "02a20965-e3c4-49aa-8f60-d969df86037f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['.', 'third', 'one', 'second', '?', 'document', 'first']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenized , removed the stop words, unique words\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "# Compute Bag of Words representation\n",
        "bow_matrix = np.zeros((len(filtered_documents), len(vocabulary)))\n",
        "for i, doc in enumerate(filtered_documents):\n",
        "    word_counts = Counter(doc)\n",
        "    for word, count in word_counts.items():\n",
        "        word_index = vocabulary.index(word)\n",
        "        bow_matrix[i, word_index] = count"
      ],
      "metadata": {
        "id": "vE3gF58m6CJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display Bag of Words matrix\n",
        "print(\"Vocabulary:\", vocabulary)\n",
        "print(\"Bag of Words Matrix:\")\n",
        "print(bow_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pBy6DQW6CMb",
        "outputId": "94c628b7-286e-4fd9-c873-35ee4e6d6099"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['.', 'third', 'one', 'second', '?', 'document', 'first']\n",
            "Bag of Words Matrix:\n",
            "[[1. 0. 0. 0. 0. 1. 1.]\n",
            " [1. 0. 0. 1. 0. 2. 0.]\n",
            " [1. 1. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 1. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# scikit learn\n",
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Sample documents\n",
        "documents = [\n",
        "    \"The movie was good and we really like it\",\n",
        "    \"The movie was good but the ending was boring \",\n",
        "    \"we did not like the movie as it was too lengthy\",\n",
        "]\n",
        "\n",
        "# Create CountVectorizer object\n",
        "vectorizer = CountVectorizer(stop_words= 'english', lowercase=True)\n",
        "\n",
        "# Fit and transform the documents\n",
        "bow_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Get the vocabulary\n",
        "vocabulary = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Convert bag of words matrix to dense array for easy printing\n",
        "bow_matrix_dense = bow_matrix.toarray()\n",
        "\n",
        "# Display Bag of Words matrix\n",
        "print(\"Vocabulary:\", vocabulary)\n",
        "print(\"Bag of Words Matrix:\")\n",
        "print(bow_matrix_dense)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdlTunEY6CP1",
        "outputId": "51e37baa-c87f-4123-b900-d00f399fd36d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['boring' 'did' 'ending' 'good' 'lengthy' 'like' 'movie' 'really']\n",
            "Bag of Words Matrix:\n",
            "[[0 0 0 1 0 1 1 1]\n",
            " [1 0 1 1 0 0 1 0]\n",
            " [0 1 0 0 1 1 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TfidfVectorizer()"
      ],
      "metadata": {
        "id": "Th9qq2Gk_euS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# Sample documents\n",
        "documents = [\n",
        "    \"The movie was good and we really like it\",\n",
        "    \"The movie was good but the ending was boring \",\n",
        "    \"we did not like the movie as it was too lengthy\",\n",
        "]\n",
        "# Create TfidfVectorizer object\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english', lowercase=True)\n",
        "# Fit and transform the documents\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
        "# Get the vocabulary\n",
        "vocabulary = tfidf_vectorizer.get_feature_names_out()\n",
        "# Convert TF-IDF matrix to dense array for easy printing\n",
        "tfidf_matrix_dense = tfidf_matrix.toarray()\n",
        "# Display TF-IDF matrix\n",
        "print(\"Vocabulary:\", vocabulary)\n",
        "print(\"TF-IDF Matrix:\")\n",
        "print(tfidf_matrix_dense)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jU_v7G1o_e5L",
        "outputId": "e7ed60a1-30e3-4896-cf14-9bc07304f550"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['boring' 'did' 'ending' 'good' 'lengthy' 'like' 'movie' 'really']\n",
            "TF-IDF Matrix:\n",
            "[[0.         0.         0.         0.4804584  0.         0.4804584\n",
            "  0.37311881 0.63174505]\n",
            " [0.5844829  0.         0.5844829  0.44451431 0.         0.\n",
            "  0.34520502 0.        ]\n",
            " [0.         0.5844829  0.         0.         0.5844829  0.44451431\n",
            "  0.34520502 0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FB4cR4qV_e8S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}